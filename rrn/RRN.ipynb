{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/andrew/Desktop/sudoku/src/sudoku')\n",
    "\n",
    "from board import Board\n",
    "from grid_string import GridString, read_solutions_file\n",
    "from shuffler import Shuffler\n",
    "from shuffled_grid import ShuffledGrid\n",
    "from solutions import Solutions\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set random seed to 0\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/Users/andrew/Desktop/sudoku/data/shuffled_puzzles.txt'\n",
    "with open(filename) as f:\n",
    "    lines = f.read().splitlines()\n",
    "puzzles = {}\n",
    "for line in lines:\n",
    "    puzzle, solution = line.split(',')\n",
    "    puzzles[GridString(puzzle)] = GridString(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_input(grid_string: GridString):\n",
    "    return torch.tensor(list(grid_string.traverse_grid()))\n",
    "\n",
    "def encode_output(grid_string: GridString):\n",
    "    return torch.tensor(list(grid_string.traverse_grid())) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_n = 10\n",
    "train_puzzles = list(puzzles.keys())[0:train_n]\n",
    "train_solutions = [puzzles[p] for p in train_puzzles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_digit = train_puzzles[0].max_digit\n",
    "num_cells = max_digit**2\n",
    "cell_vec_dim = max_digit + 1\n",
    "train_x = torch.cat([encode_input(p) for p in train_puzzles]).reshape(train_n, num_cells)\n",
    "train_y = torch.cat([encode_output(p) for p in train_solutions]).reshape(train_n, num_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_sizes = layer_sizes\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        prev_layer_size = self.layer_sizes[0]\n",
    "        for size in self.layer_sizes[1:]:\n",
    "            self.layers.append(nn.Linear(prev_layer_size, size))\n",
    "            prev_layer_size = size\n",
    "\n",
    "    def forward(self, X):\n",
    "        vector = X\n",
    "        for layer in self.layers:\n",
    "            vector = layer(vector)\n",
    "        return vector\n",
    "\n",
    "class RRN(nn.Module):\n",
    "    def __init__(self, max_digit, embed_size=16):\n",
    "        super(RRN, self).__init__()\n",
    "        self.max_digit = max_digit\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.output_size = max_digit\n",
    "        \n",
    "        self.embed_layer = nn.Embedding(self.max_digit+1, self.embed_size)\n",
    "        self.useless_layer = nn.Linear(self.embed_size, self.output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        embedding = self.embed_layer(X)\n",
    "        output = self.useless_layer(embedding)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5979, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.5876, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.5773, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.5671, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.5569, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.5468, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.5368, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.5268, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.5168, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.5069, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.4971, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.4873, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.4776, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.4679, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.4583, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.4487, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.4392, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.4297, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.4203, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.4110, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.4017, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.3925, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.3833, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.3742, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.3651, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.3561, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.3471, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.3383, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.3294, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.3206, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.3119, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.3032, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.2946, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.2860, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.2775, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.2690, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.2606, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.2523, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.2440, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.2357, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.2275, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.2194, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.2113, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.2032, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1952, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1873, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1794, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1716, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1638, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1560, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1484, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1407, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1331, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1256, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1181, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1106, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.1032, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0959, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0886, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0813, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0741, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0670, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0599, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0528, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0458, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0388, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0319, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0250, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0182, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0114, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(1.0047, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9980, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9914, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9848, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9782, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9717, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9653, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9588, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9525, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9461, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9399, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9336, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9275, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9213, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9152, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9092, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.9032, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8972, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8913, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8854, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8796, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8738, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8680, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8624, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8567, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8511, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8455, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8400, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8345, grad_fn=<NllLoss2DBackward>)\n",
      "tensor(0.8291, grad_fn=<NllLoss2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = RRN(max_digit)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(train_x)\n",
    "    prediction = prediction.permute(0,2,1)\n",
    "    loss = F.cross_entropy(prediction, train_y)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "for i in range(100):\n",
    "    print(optimizer.step(closure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
