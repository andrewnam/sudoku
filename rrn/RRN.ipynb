{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/andrew/Desktop/sudoku/src/sudoku')\n",
    "\n",
    "from board import Board\n",
    "from grid_string import GridString, read_solutions_file\n",
    "from shuffler import Shuffler\n",
    "from shuffled_grid import ShuffledGrid\n",
    "from solutions import Solutions\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set random seed to 0\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/Users/andrew/Desktop/sudoku/data/shuffled_puzzles.txt'\n",
    "with open(filename) as f:\n",
    "    lines = f.read().splitlines()\n",
    "puzzles = {}\n",
    "for line in lines:\n",
    "    puzzle, solution = line.split(',')\n",
    "    puzzles[GridString(puzzle)] = GridString(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def determine_edges(dim_x, dim_y):\n",
    "    \"\"\"\n",
    "    Returns a 2-d array of (max_digit**2, n) where the i_th entry is a list of\n",
    "        other cells' indices that cell i shares a house with\n",
    "    \"\"\"\n",
    "    max_digit = dim_x*dim_y\n",
    "    edges = []\n",
    "    for row in range(max_digit):\n",
    "        row_edges = []\n",
    "        for col in range(max_digit):\n",
    "            # row & column\n",
    "            col_edges = {(row, i) for i in range(max_digit)}\n",
    "            col_edges |= {(i, col) for i in range(max_digit)}\n",
    "            \n",
    "            # box\n",
    "            x_min = (row // dim_x) * dim_x\n",
    "            y_min = (col // dim_y) * dim_y\n",
    "            col_edges |= set(itertools.product(range(x_min, x_min+dim_x), range(y_min, y_min+dim_y)))\n",
    "            \n",
    "            # removing self\n",
    "            col_edges -= {(row, col)}\n",
    "            col_edges = [row*max_digit + col for row, col in col_edges]\n",
    "            row_edges.append(sorted(col_edges))\n",
    "        edges.append(row_edges)\n",
    "    edges = torch.tensor(edges)\n",
    "    shape = edges.shape\n",
    "    return edges.reshape(max_digit**2, shape[2])\n",
    "\n",
    "def encode_input(grid_string: GridString):\n",
    "    return torch.tensor(list(grid_string.traverse_grid()))\n",
    "\n",
    "def encode_output(grid_string: GridString):\n",
    "    return torch.tensor(list(grid_string.traverse_grid())) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_n = 10\n",
    "train_puzzles = list(puzzles.keys())[0:train_n]\n",
    "train_solutions = [puzzles[p] for p in train_puzzles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_digit = train_puzzles[0].max_digit\n",
    "num_cells = max_digit**2\n",
    "cell_vec_dim = max_digit + 1\n",
    "train_x = torch.cat([encode_input(p) for p in train_puzzles]).reshape(train_n, num_cells)\n",
    "train_y = torch.cat([encode_output(p) for p in train_solutions]).reshape(train_n, num_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_sizes = layer_sizes\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        prev_layer_size = self.layer_sizes[0]\n",
    "        for size in self.layer_sizes[1:]:\n",
    "            self.layers.append(nn.Linear(prev_layer_size, size))\n",
    "            prev_layer_size = size\n",
    "\n",
    "    def forward(self, X):\n",
    "        vector = X\n",
    "        for layer in self.layers:\n",
    "            vector = layer(vector)\n",
    "        return vector\n",
    "\n",
    "class RRN(nn.Module):\n",
    "    def __init__(self, dim_x, dim_y, embed_size=16, hidden_layer_size=96):\n",
    "        super(RRN, self).__init__()\n",
    "        self.max_digit = dim_x * dim_y\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.edges = determine_edges(dim_x, dim_y)\n",
    "\n",
    "\n",
    "        self.embed_layer = nn.Embedding(self.max_digit+1, self.embed_size)\n",
    "        self.input_mlp = MLP([self.embed_size,\n",
    "                              self.hidden_layer_size,\n",
    "                              self.hidden_layer_size,\n",
    "                              self.hidden_layer_size])\n",
    "        \n",
    "        self.f = MLP([2*self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size])\n",
    "        self.g_mlp = MLP([2*self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size])\n",
    "        self.g_lstm = nn.LSTM(self.hidden_layer_size, self.hidden_layer_size)\n",
    "        self.r = MLP([self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.max_digit])\n",
    "    \n",
    "    def compute_messages(self, H):\n",
    "        messages = torch.zeros(H.shape)\n",
    "        batch_size = H.shape[0]\n",
    "        num_nodes = H.shape[1]\n",
    "        for puzzle_index in range(batch_size): # for puzzle in batch\n",
    "            messages[puzzle_index] = torch.tensor([torch.sum(H[puzzle_index][self.edges[n]]) for n in range(num_nodes)])\n",
    "        return messages\n",
    "                    \n",
    "\n",
    "    def forward(self, grids, iters):\n",
    "        batch_size = len(grids)\n",
    "        num_nodes = self.max_digit**2\n",
    "        edges_per_nodes = self.edges.shape[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        embeddings = self.embed_layer(grids)\n",
    "        X = self.input_mlp(embeddings)\n",
    "        H = torch.tensor(X)\n",
    "        g_lstm_h = H.reshape(1, batch_size*num_nodes, self.hidden_layer_size)\n",
    "        g_lstm_c = torch.randn(1, batch_size*num_nodes, self.hidden_layer_size)\n",
    "#         g_lstm_h = torch.zeros(1, batch_size, 3)\n",
    "#         g_lstm_c = torch.zeros(1, batch_size, 3)\n",
    "        \n",
    "        outputs = []\n",
    "        for i in range(iters):\n",
    "            M = torch.zeros(batch_size, self.max_digit**2, self.hidden_layer_size)\n",
    "            for node in range(num_nodes):\n",
    "                msgs = torch.cat([self.f(torch.cat([H[:,node,:], H[:,other,:]], dim=1)) for other in self.edges[node]])\n",
    "                msgs = msgs.reshape(edges_per_nodes, batch_size, self.hidden_layer_size).permute(1,0,2)\n",
    "                M[:,node,:] = torch.sum(msgs, dim=1)\n",
    "            \n",
    "            input_to_g_lstm = self.g_mlp(torch.cat([X, M], dim=2)).reshape(1, batch_size*num_nodes, self.hidden_layer_size)\n",
    "            \n",
    "            _, (g_lstm_h, g_lstm_c) = self.g_lstm(input_to_g_lstm, (g_lstm_h, g_lstm_c))\n",
    "            H = g_lstm_h.reshape(H.shape)\n",
    "            output = self.r(H)\n",
    "            \n",
    "            outputs.append(output)\n",
    "                \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44.4412, grad_fn=<ThAddBackward>)\n",
      "tensor(44.3818, grad_fn=<ThAddBackward>)\n",
      "tensor(44.3518, grad_fn=<ThAddBackward>)\n",
      "tensor(44.3128, grad_fn=<ThAddBackward>)\n",
      "tensor(44.2705, grad_fn=<ThAddBackward>)\n",
      "tensor(44.2244, grad_fn=<ThAddBackward>)\n",
      "tensor(44.1693, grad_fn=<ThAddBackward>)\n",
      "tensor(44.0981, grad_fn=<ThAddBackward>)\n",
      "tensor(43.9988, grad_fn=<ThAddBackward>)\n",
      "tensor(43.8766, grad_fn=<ThAddBackward>)\n",
      "tensor(43.6967, grad_fn=<ThAddBackward>)\n",
      "tensor(43.4436, grad_fn=<ThAddBackward>)\n",
      "tensor(43.0729, grad_fn=<ThAddBackward>)\n",
      "tensor(42.5947, grad_fn=<ThAddBackward>)\n",
      "tensor(41.9515, grad_fn=<ThAddBackward>)\n",
      "tensor(41.1321, grad_fn=<ThAddBackward>)\n",
      "tensor(40.1081, grad_fn=<ThAddBackward>)\n",
      "tensor(39.0531, grad_fn=<ThAddBackward>)\n",
      "tensor(38.1158, grad_fn=<ThAddBackward>)\n",
      "tensor(37.3865, grad_fn=<ThAddBackward>)\n",
      "tensor(36.7801, grad_fn=<ThAddBackward>)\n",
      "tensor(36.4099, grad_fn=<ThAddBackward>)\n",
      "tensor(35.6825, grad_fn=<ThAddBackward>)\n",
      "tensor(35.0393, grad_fn=<ThAddBackward>)\n",
      "tensor(34.1237, grad_fn=<ThAddBackward>)\n",
      "tensor(33.0995, grad_fn=<ThAddBackward>)\n",
      "tensor(32.2107, grad_fn=<ThAddBackward>)\n",
      "tensor(30.7657, grad_fn=<ThAddBackward>)\n",
      "tensor(29.3276, grad_fn=<ThAddBackward>)\n",
      "tensor(27.8732, grad_fn=<ThAddBackward>)\n",
      "tensor(27.2482, grad_fn=<ThAddBackward>)\n",
      "tensor(26.3909, grad_fn=<ThAddBackward>)\n",
      "tensor(25.5754, grad_fn=<ThAddBackward>)\n",
      "tensor(25.0065, grad_fn=<ThAddBackward>)\n",
      "tensor(25.2017, grad_fn=<ThAddBackward>)\n",
      "tensor(24.1417, grad_fn=<ThAddBackward>)\n",
      "tensor(22.8198, grad_fn=<ThAddBackward>)\n",
      "tensor(22.7622, grad_fn=<ThAddBackward>)\n",
      "tensor(21.8656, grad_fn=<ThAddBackward>)\n",
      "tensor(21.6281, grad_fn=<ThAddBackward>)\n",
      "tensor(21.4956, grad_fn=<ThAddBackward>)\n",
      "tensor(20.9447, grad_fn=<ThAddBackward>)\n",
      "tensor(20.6544, grad_fn=<ThAddBackward>)\n",
      "tensor(20.1401, grad_fn=<ThAddBackward>)\n",
      "tensor(19.4117, grad_fn=<ThAddBackward>)\n",
      "tensor(19.2586, grad_fn=<ThAddBackward>)\n",
      "tensor(18.5637, grad_fn=<ThAddBackward>)\n",
      "tensor(18.2425, grad_fn=<ThAddBackward>)\n",
      "tensor(17.8482, grad_fn=<ThAddBackward>)\n",
      "tensor(17.7508, grad_fn=<ThAddBackward>)\n",
      "tensor(16.8846, grad_fn=<ThAddBackward>)\n",
      "tensor(16.1767, grad_fn=<ThAddBackward>)\n",
      "tensor(15.6090, grad_fn=<ThAddBackward>)\n",
      "tensor(15.4270, grad_fn=<ThAddBackward>)\n",
      "tensor(15.6225, grad_fn=<ThAddBackward>)\n",
      "tensor(14.9066, grad_fn=<ThAddBackward>)\n",
      "tensor(14.1960, grad_fn=<ThAddBackward>)\n",
      "tensor(13.9149, grad_fn=<ThAddBackward>)\n",
      "tensor(12.8566, grad_fn=<ThAddBackward>)\n",
      "tensor(12.2768, grad_fn=<ThAddBackward>)\n",
      "tensor(11.8241, grad_fn=<ThAddBackward>)\n",
      "tensor(11.1435, grad_fn=<ThAddBackward>)\n",
      "tensor(9.9951, grad_fn=<ThAddBackward>)\n",
      "tensor(9.6652, grad_fn=<ThAddBackward>)\n",
      "tensor(8.9712, grad_fn=<ThAddBackward>)\n",
      "tensor(8.5040, grad_fn=<ThAddBackward>)\n",
      "tensor(7.9064, grad_fn=<ThAddBackward>)\n",
      "tensor(7.0393, grad_fn=<ThAddBackward>)\n",
      "tensor(6.7988, grad_fn=<ThAddBackward>)\n",
      "tensor(6.2001, grad_fn=<ThAddBackward>)\n",
      "tensor(5.0915, grad_fn=<ThAddBackward>)\n",
      "tensor(4.4849, grad_fn=<ThAddBackward>)\n",
      "tensor(4.5500, grad_fn=<ThAddBackward>)\n",
      "tensor(3.9787, grad_fn=<ThAddBackward>)\n",
      "tensor(3.4963, grad_fn=<ThAddBackward>)\n",
      "tensor(3.3106, grad_fn=<ThAddBackward>)\n",
      "tensor(2.8642, grad_fn=<ThAddBackward>)\n",
      "tensor(3.1227, grad_fn=<ThAddBackward>)\n",
      "tensor(2.7196, grad_fn=<ThAddBackward>)\n",
      "tensor(2.5549, grad_fn=<ThAddBackward>)\n",
      "tensor(2.5161, grad_fn=<ThAddBackward>)\n",
      "tensor(2.6545, grad_fn=<ThAddBackward>)\n",
      "tensor(2.2786, grad_fn=<ThAddBackward>)\n",
      "tensor(2.0093, grad_fn=<ThAddBackward>)\n",
      "tensor(2.0560, grad_fn=<ThAddBackward>)\n",
      "tensor(1.7722, grad_fn=<ThAddBackward>)\n",
      "tensor(1.4654, grad_fn=<ThAddBackward>)\n",
      "tensor(2.2284, grad_fn=<ThAddBackward>)\n",
      "tensor(1.7365, grad_fn=<ThAddBackward>)\n",
      "tensor(1.4035, grad_fn=<ThAddBackward>)\n",
      "tensor(1.4840, grad_fn=<ThAddBackward>)\n",
      "tensor(1.5600, grad_fn=<ThAddBackward>)\n",
      "tensor(1.6681, grad_fn=<ThAddBackward>)\n",
      "tensor(1.2624, grad_fn=<ThAddBackward>)\n",
      "tensor(1.4091, grad_fn=<ThAddBackward>)\n",
      "tensor(1.4139, grad_fn=<ThAddBackward>)\n",
      "tensor(1.3800, grad_fn=<ThAddBackward>)\n",
      "tensor(1.2533, grad_fn=<ThAddBackward>)\n",
      "tensor(1.5136, grad_fn=<ThAddBackward>)\n",
      "tensor(0.9800, grad_fn=<ThAddBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RRN( dim_x=2, dim_y=2, embed_size=4, hidden_layer_size=32)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    predictions = [p.permute(0,2,1) for p in model(train_x, 32)]\n",
    "#     print([F.cross_entropy(p, train_y) for p in predictions])\n",
    "    loss = sum([F.cross_entropy(p, train_y) for p in predictions])\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "for i in tqdm_notebook(range(100)):\n",
    "    print(optimizer.step(closure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cell in determine_edges(2, 2):\n",
    "    print(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(3, 1), (3, 2), (3, 3), (4, 1), (4, 2), (4, 3), (5, 1), (5, 2), (5, 3)}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(itertools.product(range(3, 6), range(1, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1234341223414123"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_solutions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(range(2*3*4)).reshape(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.tensor(range(2*3*4)).reshape(2,3,4)\n",
    "z = torch.zeros(m.shape)\n",
    "\n",
    "\n",
    "# print(m[0])\n",
    "# print(m[0][0])\n",
    "\n",
    "# m = torch.cat([m]*2)\n",
    "# m = m.reshape(2,2,3,4)\n",
    "# m = m.permute(1,2,0,3)\n",
    "# print(m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6,  7],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "        [12, 13, 14, 15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([m[:,0,:], m[:,1,:]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = torch.zeros(batch_size, self.max_digit**2, self.hidden_layer_size)\n",
    "for node in range(num_nodes):\n",
    "    M[:,n,:] = torch.sum([self.f(torch.cat([H[:,node,:], H[:,other,:]], dim=1)) for other in self.edges[node]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]],\n",
       "\n",
       "        [[20, 21, 22, 23, 24],\n",
       "         [25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34],\n",
       "         [35, 36, 37, 38, 39]],\n",
       "\n",
       "        [[40, 41, 42, 43, 44],\n",
       "         [45, 46, 47, 48, 49],\n",
       "         [50, 51, 52, 53, 54],\n",
       "         [55, 56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.tensor(range(3*4*5)).reshape(3,4,5)\n",
    "e = torch.tensor([[0,1], [1,2], [2,3], [3,0]])\n",
    "z = torch.zeros(3,4,5)\n",
    "for n in range(4):\n",
    "    # reshape()\n",
    "    a = torch.cat([m[:,other] for other in e[n]]).reshape(2, 3, 5).permute(1,0,2)\n",
    "    z[:,n,:] = torch.sum(a, dim=1)\n",
    "    \n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.,   7.,   9.,  11.,  13.],\n",
       "         [ 15.,  17.,  19.,  21.,  23.],\n",
       "         [ 25.,  27.,  29.,  31.,  33.],\n",
       "         [ 15.,  17.,  19.,  21.,  23.]],\n",
       "\n",
       "        [[ 45.,  47.,  49.,  51.,  53.],\n",
       "         [ 55.,  57.,  59.,  61.,  63.],\n",
       "         [ 65.,  67.,  69.,  71.,  73.],\n",
       "         [ 55.,  57.,  59.,  61.,  63.]],\n",
       "\n",
       "        [[ 85.,  87.,  89.,  91.,  93.],\n",
       "         [ 95.,  97.,  99., 101., 103.],\n",
       "         [105., 107., 109., 111., 113.],\n",
       "         [ 95.,  97.,  99., 101., 103.]]])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {
    "c40c337e0acc49978734a2ec9abd19aa": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
