{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "SUDOKU_PATH = '/home/ajhnam/sudoku'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cutorch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append(SUDOKU_PATH + '/src/sudoku')\n",
    "\n",
    "from board import Board\n",
    "from grid_string import GridString, read_solutions_file\n",
    "from shuffler import Shuffler\n",
    "from shuffled_grid import ShuffledGrid\n",
    "from solutions import Solutions\n",
    "from dataset import Dataset\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed to 0\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_model_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_tensor_memory_size(tensor):\n",
    "    return tensor.element_size() * tensor.nelement() // (2**20)\n",
    "\n",
    "def get_gpu_memory(device):\n",
    "    return cutorch.memory_allocated(device) // (2**20)\n",
    "\n",
    "def determine_edges(dim_x, dim_y):\n",
    "    \"\"\"\n",
    "    Returns a 2-d array of (max_digit**2, n) where the i_th entry is a list of\n",
    "        other cells' indices that cell i shares a house with\n",
    "    \"\"\"\n",
    "    max_digit = dim_x*dim_y\n",
    "    edges = []\n",
    "    for row in range(max_digit):\n",
    "        row_edges = []\n",
    "        for col in range(max_digit):\n",
    "            # row & column\n",
    "            col_edges = {(row, i) for i in range(max_digit)}\n",
    "            col_edges |= {(i, col) for i in range(max_digit)}\n",
    "            \n",
    "            # box\n",
    "            x_min = (row // dim_x) * dim_x\n",
    "            y_min = (col // dim_y) * dim_y\n",
    "            col_edges |= set(itertools.product(range(x_min, x_min+dim_x), range(y_min, y_min+dim_y)))\n",
    "            \n",
    "            # removing self\n",
    "            col_edges -= {(row, col)}\n",
    "            col_edges = [row*max_digit + col for row, col in col_edges]\n",
    "            row_edges.append(sorted(col_edges))\n",
    "        edges.append(row_edges)\n",
    "    edges = torch.tensor(edges)\n",
    "    shape = edges.shape\n",
    "    return edges.reshape(max_digit**2, shape[2])\n",
    "\n",
    "def encode_input(grid_string: GridString):\n",
    "    return torch.tensor(list(grid_string.traverse_grid()))\n",
    "\n",
    "def encode_output(grid_string: GridString):\n",
    "    return torch.tensor(list(grid_string.traverse_grid())) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = Dataset.load(SUDOKU_PATH + '/data/puzzles.dst')\n",
    "\n",
    "max_digit = 4\n",
    "num_cells = max_digit**2\n",
    "cell_vec_dim = max_digit + 1\n",
    "train_inputs = dst.get_input_data(0)\n",
    "train_outputs = dst.get_output_data(0)\n",
    "train_x = torch.cat([encode_input(p) for p in train_inputs]).reshape(len(train_inputs), num_cells)#.cuda(device)\n",
    "train_y = torch.cat([encode_output(p) for p in train_outputs]).reshape(len(train_outputs), num_cells)#.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_sizes = layer_sizes\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        \n",
    "        prev_layer_size = self.layer_sizes[0]\n",
    "        for size in self.layer_sizes[1:]:\n",
    "            self.layers.append(nn.Linear(prev_layer_size, size))\n",
    "            prev_layer_size = size\n",
    "\n",
    "    def forward(self, X):\n",
    "        vector = X\n",
    "        for layer in self.layers[:-1]:\n",
    "            vector = self.nonlinear(layer(vector))\n",
    "        return self.layers[-1](vector)\n",
    "\n",
    "class RRN(nn.Module):\n",
    "    def __init__(self, dim_x, dim_y, embed_size=16, hidden_layer_size=96):\n",
    "        super(RRN, self).__init__()\n",
    "        self.max_digit = dim_x * dim_y\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.edges = determine_edges(dim_x, dim_y)\n",
    "\n",
    "\n",
    "        self.embed_layer = nn.Embedding(self.max_digit+1, self.embed_size)\n",
    "        self.input_mlp = MLP([self.embed_size,\n",
    "                              self.hidden_layer_size,\n",
    "                              self.hidden_layer_size,\n",
    "                              self.hidden_layer_size])\n",
    "        \n",
    "        self.f = MLP([2*self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size])\n",
    "        self.g_mlp = MLP([2*self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size])\n",
    "        self.g_lstm = nn.LSTM(self.hidden_layer_size, self.hidden_layer_size)\n",
    "        self.r = MLP([self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.max_digit])\n",
    "    \n",
    "    def compute_messages(self, H):\n",
    "        messages = torch.zeros(H.shape)\n",
    "        batch_size = H.shape[0]\n",
    "        num_nodes = H.shape[1]\n",
    "        for puzzle_index in range(batch_size): # for puzzle in batch\n",
    "            messages[puzzle_index] = torch.tensor([torch.sum(H[puzzle_index][self.edges[n]]) for n in range(num_nodes)]).cuda(H.get_device())\n",
    "        return messages\n",
    "                    \n",
    "\n",
    "    def forward(self, grids, iters):\n",
    "        batch_size = len(grids)\n",
    "        num_nodes = self.max_digit**2\n",
    "        edges_per_nodes = self.edges.shape[1]\n",
    "        \n",
    "        embeddings = self.embed_layer(grids)\n",
    "        X = self.input_mlp(embeddings)\n",
    "        H = torch.tensor(X).cuda(grids.get_device())\n",
    "        g_lstm_h = H.reshape(1, batch_size*num_nodes, self.hidden_layer_size)\n",
    "        g_lstm_c = torch.randn(1, batch_size*num_nodes, self.hidden_layer_size).cuda(grids.get_device())\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(iters):\n",
    "            M = torch.zeros(batch_size, self.max_digit**2, self.hidden_layer_size).cuda(grids.get_device())\n",
    "            for node in range(num_nodes):\n",
    "                msgs = torch.cat([self.f(torch.cat([H[:,node,:], H[:,other,:]], dim=1)) for other in self.edges[node]])\n",
    "                msgs = msgs.reshape(edges_per_nodes, batch_size, self.hidden_layer_size).permute(1,0,2)\n",
    "                M[:,node,:] = torch.sum(msgs, dim=1)\n",
    "            \n",
    "            input_to_g_lstm = self.g_mlp(torch.cat([X, M], dim=2)).reshape(1, batch_size*num_nodes, self.hidden_layer_size)\n",
    "            \n",
    "            , (g_lstm_h, g_lstm_c) = self.g_lstm(input_to_g_lstm, (g_lstm_h, g_lstm_c))\n",
    "            H = g_lstm_h.reshape(H.shape)\n",
    "            output = self.r(H)\n",
    "            outputs.append(output)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5996d3b0b64a7daaa5579cefec0282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajhnam/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 | Device 0 | Memory 0 MB | Loss 44.40128\n",
      "Iter 1 | Device 0 | Memory 0 MB | Loss 44.36824\n",
      "Iter 2 | Device 0 | Memory 0 MB | Loss 44.3356\n",
      "Iter 3 | Device 0 | Memory 0 MB | Loss 44.30364\n",
      "Iter 4 | Device 0 | Memory 0 MB | Loss 44.27222\n",
      "Iter 5 | Device 0 | Memory 0 MB | Loss 44.24123\n",
      "Iter 6 | Device 0 | Memory 0 MB | Loss 44.21095\n",
      "Iter 7 | Device 0 | Memory 0 MB | Loss 44.18199\n",
      "Iter 8 | Device 0 | Memory 0 MB | Loss 44.15249\n",
      "Iter 9 | Device 0 | Memory 0 MB | Loss 44.12293\n",
      "Iter 10 | Device 0 | Memory 0 MB | Loss 44.09279\n",
      "Iter 11 | Device 0 | Memory 0 MB | Loss 44.06256\n",
      "Iter 12 | Device 0 | Memory 0 MB | Loss 44.03141\n",
      "Iter 13 | Device 0 | Memory 0 MB | Loss 43.99917\n",
      "Iter 14 | Device 0 | Memory 0 MB | Loss 43.96599\n",
      "Iter 15 | Device 0 | Memory 0 MB | Loss 43.93149\n",
      "Iter 16 | Device 0 | Memory 0 MB | Loss 43.89633\n",
      "Iter 17 | Device 0 | Memory 0 MB | Loss 43.86076\n",
      "Iter 18 | Device 0 | Memory 0 MB | Loss 43.82517\n",
      "Iter 19 | Device 0 | Memory 0 MB | Loss 43.79014\n",
      "Iter 20 | Device 0 | Memory 0 MB | Loss 43.7531\n",
      "Iter 21 | Device 0 | Memory 0 MB | Loss 43.71491\n",
      "Iter 22 | Device 0 | Memory 0 MB | Loss 43.67502\n",
      "Iter 23 | Device 0 | Memory 0 MB | Loss 43.63369\n",
      "Iter 24 | Device 0 | Memory 0 MB | Loss 43.59311\n",
      "\n",
      "23.922709703445435\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "device = 0\n",
    "\n",
    "x_batch = train_x.cuda(device)\n",
    "y_batch = train_y.cuda(device)\n",
    "\n",
    "model = RRN( dim_x=2, dim_y=2, embed_size=6, hidden_layer_size=32).cuda(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "num_iters = 32\n",
    "epochs = 25\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    for j, p in zip(range(num_iters), model(x_batch, num_iters)):\n",
    "        loss = F.cross_entropy(p.permute(0,2,1), y_batch)\n",
    "        total_loss += loss\n",
    "    total_loss.backward()\n",
    "    return total_loss\n",
    "\n",
    "for i in tqdm_notebook(range(epochs)):\n",
    "    loss = optimizer.step(closure)\n",
    "    print(\"Iter {} | Device {} | Memory {} MB | Loss {}\".format(i, device, get_gpu_memory(device), round(float(loss), 5)))\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e84fca2e7bd43f587f1b02c40434048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajhnam/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 | Loss 44.43356\n",
      "Iter 1 | Loss 44.4025\n",
      "Iter 2 | Loss 44.38362\n",
      "Iter 3 | Loss 44.36824\n",
      "Iter 4 | Loss 44.3536\n",
      "Iter 5 | Loss 44.34033\n",
      "Iter 6 | Loss 44.32745\n",
      "Iter 7 | Loss 44.31503\n",
      "Iter 8 | Loss 44.30307\n",
      "Iter 9 | Loss 44.29182\n",
      "Iter 10 | Loss 44.28115\n",
      "Iter 11 | Loss 44.27115\n",
      "Iter 12 | Loss 44.26207\n",
      "Iter 13 | Loss 44.25346\n",
      "Iter 14 | Loss 44.24501\n",
      "Iter 15 | Loss 44.23696\n",
      "Iter 16 | Loss 44.22907\n",
      "Iter 17 | Loss 44.22157\n",
      "Iter 18 | Loss 44.21484\n",
      "Iter 19 | Loss 44.20761\n",
      "Iter 20 | Loss 44.20038\n",
      "Iter 21 | Loss 44.1931\n",
      "Iter 22 | Loss 44.18606\n",
      "Iter 23 | Loss 44.17916\n",
      "Iter 24 | Loss 44.1723\n",
      "\n",
      "54.17464590072632\n"
     ]
    }
   ],
   "source": [
    "device_ids = [0,2]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "device = 0\n",
    "\n",
    "x_batch = train_x.cuda(device)\n",
    "y_batch = train_y.cuda(device)\n",
    "\n",
    "model = RRN( dim_x=2, dim_y=2, embed_size=6, hidden_layer_size=32).cuda(device)\n",
    "model = torch.nn.DataParallel(model, device_ids=device_ids).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "num_iters = 32\n",
    "epochs = 25\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    for j, p in zip(range(num_iters), model(x_batch, num_iters)):\n",
    "        loss = F.cross_entropy(p.permute(0,2,1), y_batch)\n",
    "        total_loss += loss\n",
    "    total_loss.backward()\n",
    "    return total_loss\n",
    "\n",
    "for i in tqdm_notebook(range(epochs)):\n",
    "    loss = optimizer.step(closure)\n",
    "    print(\"Iter {} | Loss {}\".format(i, round(float(loss), 5)))\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
