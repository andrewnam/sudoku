{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/andrew/Desktop/sudoku/src/sudoku')\n",
    "\n",
    "from board import Board\n",
    "from solutions import Solutions\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1090f7c70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed to 0\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_encode(board_string):\n",
    "    dim_x, dim_y, board = board_string.split('.')\n",
    "    max_digit = int(dim_x) * int(dim_y)\n",
    "    vector = np.zeros((max_digit*len(board),), dtype=np.float64)\n",
    "    for i in range(len(board)):\n",
    "        if board[i] != '0':\n",
    "            vector[i*max_digit + int(board[i]) - 1] = 1\n",
    "        else:\n",
    "            vector[i*max_digit:(i+1)*max_digit] = 1/max_digit\n",
    "    return vector\n",
    "\n",
    "def get_board_entries(board_string):\n",
    "    return np.array(list(board_string[4:]), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, max_digit):\n",
    "        super(Linear, self).__init__()\n",
    "        self.max_digit = max_digit\n",
    "        self.linear = nn.Linear(self.max_digit**3, self.max_digit**3)\n",
    "\n",
    "    def forward(self, X):\n",
    "        pre_output = self.linear(X)\n",
    "        predictions = [softmax(pre_output[i*self.max_digit:(i+1)*self.max_digit]) for i in range(self.max_digit**2)]\n",
    "        return torch.stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_digit = 4\n",
    "model = Linear(max_digit)\n",
    "model.double()\n",
    "\n",
    "optimizer = optim.LBFGS(model.parameters(), lr=0.8)\n",
    "X = torch.tensor(vector_encode('2.2.0134432100400010'))\n",
    "Y = torch.tensor(get_board_entries('2.2.2134432112433412'), dtype=torch.int64) - 1\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(X)\n",
    "    loss = nn.functional.nll_loss(prediction, Y)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2535, grad_fn=<NllLossBackward>)\n",
      "tensor(-0.2580, grad_fn=<NllLossBackward>)\n",
      "tensor(-0.2877, grad_fn=<NllLossBackward>)\n",
      "tensor(-0.3215, grad_fn=<NllLossBackward>)\n",
      "tensor(-0.3593, grad_fn=<NllLossBackward>)\n",
      "tensor(-0.4009, grad_fn=<NllLossBackward>)\n",
      "tensor(-0.4454, grad_fn=<NllLossBackward>)\n",
      "tensor(-0.4919, grad_fn=<NllLossBackward>)\n",
      "tensor(-0.5388, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n",
      "tensor(-1.0000, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solutions = Solutions('/Users/andrew/Desktop/sudoku/data/solutions5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "puzzles = solutions.get_puzzles_by_hints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 12\n",
      "15 188\n",
      "14 716\n",
      "13 972\n",
      "12 899\n",
      "11 824\n",
      "10 711\n",
      "9 533\n",
      "8 339\n",
      "7 230\n",
      "6 135\n",
      "5 58\n",
      "4 10\n"
     ]
    }
   ],
   "source": [
    "for hints in puzzles:\n",
    "    print(hints, len(puzzles[hints]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.0000, 0.0000, 9.0000, 0.0000]], requires_grad=True)\n",
      "tensor([0])\n",
      "tensor(-0.2000, grad_fn=<NllLossBackward>)\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(np.array([[1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1]]), dtype=torch.float64)\n",
    "b = torch.tensor(np.array([0, 2, 4]), dtype=torch.int64)\n",
    "a = torch.tensor(np.array([[.2, 0, 0, 0, 0]]), dtype=torch.float64, requires_grad=True)\n",
    "b = torch.tensor(np.array([0]), dtype=torch.int64)\n",
    "loss = nn.functional.nll_loss(a, b)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(loss)\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn.functional.nll_loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    print('STEP: ', i)\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = seq(input)\n",
    "        loss = criterion(out, target)\n",
    "        print('loss:', loss.item())\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(closure)\n",
    "    # begin to predict, no need to track gradient here\n",
    "    with torch.no_grad():\n",
    "        future = 1000\n",
    "        pred = seq(test_input, future=future)\n",
    "        loss = criterion(pred[:, :-future], test_target)\n",
    "        print('test loss:', loss.item())\n",
    "        y = pred.detach().numpy()\n",
    "    # draw the result\n",
    "    plt.figure(figsize=(30,10))\n",
    "    plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
    "    plt.xlabel('x', fontsize=20)\n",
    "    plt.ylabel('y', fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    def draw(yi, color):\n",
    "        plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth = 2.0)\n",
    "        plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth = 2.0)\n",
    "    draw(y[0], 'r')\n",
    "    draw(y[1], 'g')\n",
    "    draw(y[2], 'b')\n",
    "    plt.savefig('predict%d.pdf'%i)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
