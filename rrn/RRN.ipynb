{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/andrew/Desktop/sudoku/src/sudoku')\n",
    "\n",
    "from board import Board\n",
    "from grid_string import GridString, read_solutions_file\n",
    "from shuffler import Shuffler\n",
    "from shuffled_grid import ShuffledGrid\n",
    "from solutions import Solutions\n",
    "from dataset import Dataset\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set random seed to 0\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '/Users/andrew/Desktop/sudoku/data/shuffled_puzzles.txt'\n",
    "with open(filename) as f:\n",
    "    lines = f.read().splitlines()\n",
    "puzzles = {}\n",
    "for line in lines:\n",
    "    puzzle, solution = line.split(',')\n",
    "    puzzles[GridString(puzzle)] = GridString(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def determine_edges(dim_x, dim_y):\n",
    "    \"\"\"\n",
    "    Returns a 2-d array of (max_digit**2, n) where the i_th entry is a list of\n",
    "        other cells' indices that cell i shares a house with\n",
    "    \"\"\"\n",
    "    max_digit = dim_x*dim_y\n",
    "    edges = []\n",
    "    for row in range(max_digit):\n",
    "        row_edges = []\n",
    "        for col in range(max_digit):\n",
    "            # row & column\n",
    "            col_edges = {(row, i) for i in range(max_digit)}\n",
    "            col_edges |= {(i, col) for i in range(max_digit)}\n",
    "            \n",
    "            # box\n",
    "            x_min = (row // dim_x) * dim_x\n",
    "            y_min = (col // dim_y) * dim_y\n",
    "            col_edges |= set(itertools.product(range(x_min, x_min+dim_x), range(y_min, y_min+dim_y)))\n",
    "            \n",
    "            # removing self\n",
    "            col_edges -= {(row, col)}\n",
    "            col_edges = [row*max_digit + col for row, col in col_edges]\n",
    "            row_edges.append(sorted(col_edges))\n",
    "        edges.append(row_edges)\n",
    "    edges = torch.tensor(edges)\n",
    "    shape = edges.shape\n",
    "    return edges.reshape(max_digit**2, shape[2])\n",
    "\n",
    "def encode_input(grid_string: GridString):\n",
    "    return torch.tensor(list(grid_string.traverse_grid()))\n",
    "\n",
    "def encode_output(grid_string: GridString):\n",
    "    return torch.tensor(list(grid_string.traverse_grid())) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_digit = 4\n",
    "dst = dataset.load('/Users/andrew/Desktop/sudoku/data/puzzles.dst')\n",
    "\n",
    "num_cells = max_digit**2\n",
    "cell_vec_dim = max_digit + 1\n",
    "train_inputs = dst.get_input_data(0)\n",
    "train_outputs = dst.get_output_data(0)\n",
    "train_x = torch.cat([encode_input(p) for p in train_inputs]).reshape(len(train_inputs), num_cells)\n",
    "train_y = torch.cat([encode_output(p) for p in train_outputs]).reshape(len(train_outputs), num_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_sizes = layer_sizes\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        prev_layer_size = self.layer_sizes[0]\n",
    "        for size in self.layer_sizes[1:]:\n",
    "            self.layers.append(nn.Linear(prev_layer_size, size))\n",
    "            prev_layer_size = size\n",
    "\n",
    "    def forward(self, X):\n",
    "        vector = X\n",
    "        for layer in self.layers:\n",
    "            vector = layer(vector)\n",
    "        return vector\n",
    "\n",
    "class RRN(nn.Module):\n",
    "    def __init__(self, dim_x, dim_y, embed_size=16, hidden_layer_size=96):\n",
    "        super(RRN, self).__init__()\n",
    "        self.max_digit = dim_x * dim_y\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.edges = determine_edges(dim_x, dim_y)\n",
    "\n",
    "\n",
    "        self.embed_layer = nn.Embedding(self.max_digit+1, self.embed_size)\n",
    "        self.input_mlp = MLP([self.embed_size,\n",
    "                              self.hidden_layer_size,\n",
    "                              self.hidden_layer_size,\n",
    "                              self.hidden_layer_size])\n",
    "        \n",
    "        self.f = MLP([2*self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size])\n",
    "        self.g_mlp = MLP([2*self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size])\n",
    "        self.g_lstm = nn.LSTM(self.hidden_layer_size, self.hidden_layer_size)\n",
    "        self.r = MLP([self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.hidden_layer_size,\n",
    "                      self.max_digit])\n",
    "    \n",
    "    def compute_messages(self, H):\n",
    "        messages = torch.zeros(H.shape)\n",
    "        batch_size = H.shape[0]\n",
    "        num_nodes = H.shape[1]\n",
    "        for puzzle_index in range(batch_size): # for puzzle in batch\n",
    "            messages[puzzle_index] = torch.tensor([torch.sum(H[puzzle_index][self.edges[n]]) for n in range(num_nodes)])\n",
    "        return messages\n",
    "                    \n",
    "\n",
    "    def forward(self, grids, iters):\n",
    "        batch_size = len(grids)\n",
    "        num_nodes = self.max_digit**2\n",
    "        edges_per_nodes = self.edges.shape[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        embeddings = self.embed_layer(grids)\n",
    "        X = self.input_mlp(embeddings)\n",
    "        H = torch.tensor(X)\n",
    "        g_lstm_h = H.reshape(1, batch_size*num_nodes, self.hidden_layer_size)\n",
    "        g_lstm_c = torch.randn(1, batch_size*num_nodes, self.hidden_layer_size)\n",
    "        \n",
    "        outputs = []\n",
    "        for i in range(iters):\n",
    "            M = torch.zeros(batch_size, self.max_digit**2, self.hidden_layer_size)\n",
    "            for node in range(num_nodes):\n",
    "                msgs = torch.cat([self.f(torch.cat([H[:,node,:], H[:,other,:]], dim=1)) for other in self.edges[node]])\n",
    "                msgs = msgs.reshape(edges_per_nodes, batch_size, self.hidden_layer_size).permute(1,0,2)\n",
    "                M[:,node,:] = torch.sum(msgs, dim=1)\n",
    "            \n",
    "            input_to_g_lstm = self.g_mlp(torch.cat([X, M], dim=2)).reshape(1, batch_size*num_nodes, self.hidden_layer_size)\n",
    "            \n",
    "            _, (g_lstm_h, g_lstm_c) = self.g_lstm(input_to_g_lstm, (g_lstm_h, g_lstm_c))\n",
    "            H = g_lstm_h.reshape(H.shape)\n",
    "            output = self.r(H)\n",
    "            \n",
    "            outputs.append(output)\n",
    "                \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RRN( dim_x=2, dim_y=2, embed_size=4, hidden_layer_size=32)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    predictions = [p.permute(0,2,1) for p in model(train_x, 32)]\n",
    "    loss = sum([F.cross_entropy(p, train_y) for p in predictions])\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "for i in tqdm_notebook(range(100)):\n",
    "    print(optimizer.step(closure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {
    "8cafc2b7aa5244a3a92d0964da193c19": {
     "views": [
      {
       "cell_index": 6
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
